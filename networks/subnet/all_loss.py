from torch.autograd import Function
import torch.nn as nn
import torch
import torch.nn.functional as F
"""
Adapted from https://github.com/fungtion/DSN/blob/master/functions.py
"""


class FeatureAlignmentLoss(nn.Module):
    def __init__(self, reduction='mean'):
        super(FeatureAlignmentLoss, self).__init__()
        self.reduction = reduction

    def forward(self, features_a, features_b):
        # 计算余弦相似度
        sim_matrix = F.cosine_similarity(features_a.unsqueeze(1), features_b.unsqueeze(0),
                                         dim=-1)  # [batch_size_a, batch_size_b]

        # 计算对齐损失，这里以负相似度为例
        loss = -sim_matrix.mean() if self.reduction == 'mean' else -sim_matrix.sum()

        return loss

class ReverseLayerF(Function):

    @staticmethod
    def forward(ctx, x, p):
        ctx.p = p

        return x.view_as(x)

    @staticmethod
    def backward(ctx, grad_output):
        output = grad_output.neg() * ctx.p

        return output, None


class MSE(nn.Module):
    def __init__(self):
        super(MSE, self).__init__()

    def forward(self, pred, real):
        diffs = torch.add(real, -pred)
        n = torch.numel(diffs.data)
        mse = torch.sum(diffs.pow(2)) / n

        return mse


class SIMSE(nn.Module):

    def __init__(self):
        super(SIMSE, self).__init__()

    def forward(self, pred, real):
        diffs = torch.add(real, - pred)
        n = torch.numel(diffs.data)
        simse = torch.sum(diffs).pow(2) / (n ** 2)

        return simse

class diss_loss(nn.Module):
    def __init__(self):
        super(diss_loss, self).__init__()  # 初始化父类
        self.di_loss = DiffLoss()  # 假设 DiffLoss 已定义

    # 注意拼写为 forward
    def forward(self, fused_text, fus_visual, relate_model_v_a, relate_model_t_a):
        """计算多模态损失"""
        loss = self.di_loss(fused_text, relate_model_v_a)
        # loss += self.di_loss(fus_audio, relate_model_v_t)
        loss += self.di_loss(fus_visual, relate_model_t_a)

        return loss

class DiffLoss(nn.Module):

    def __init__(self):
        super(DiffLoss, self).__init__()

    def forward(self, input1, input2):
        batch_size = input1.size(0)
        input1 = input1.view(batch_size, -1)
        input2 = input2.view(batch_size, -1)

        # Zero mean
        input1_mean = torch.mean(input1, dim=0, keepdims=True)
        input2_mean = torch.mean(input2, dim=0, keepdims=True)
        input1 = input1 - input1_mean
        input2 = input2 - input2_mean

        input1_l2_norm = torch.norm(input1, p=2, dim=1, keepdim=True).detach()
        input1_l2 = input1.div(input1_l2_norm.expand_as(input1) + 1e-6)

        input2_l2_norm = torch.norm(input2, p=2, dim=1, keepdim=True).detach()
        input2_l2 = input2.div(input2_l2_norm.expand_as(input2) + 1e-6)

        diff_loss = torch.mean((input1_l2.t().mm(input2_l2)).pow(2))

        return diff_loss


class CMD(nn.Module):
    """
    Adapted from https://github.com/wzell/cmd/blob/master/models/domain_regularizer.py
    """

    def __init__(self):
        super(CMD, self).__init__()

    def forward(self, x1, x2, n_moments):
        mx1 = torch.mean(x1, 0)
        mx2 = torch.mean(x2, 0)
        sx1 = x1 - mx1
        sx2 = x2 - mx2
        dm = self.matchnorm(mx1, mx2)
        scms = dm
        for i in range(n_moments - 1):
            scms += self.scm(sx1, sx2, i + 2)
        return scms

    def matchnorm(self, x1, x2):
        power = torch.pow(x1 - x2, 2)
        summed = torch.sum(power)
        sqrt = summed ** (0.5)
        return sqrt
        # return ((x1-x2)**2).sum().sqrt()

    def scm(self, sx1, sx2, k):
        ss1 = torch.mean(torch.pow(sx1, k), 0)
        ss2 = torch.mean(torch.pow(sx2, k), 0)
        return self.matchnorm(ss1, ss2)
